{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from datasets import load_dataset\n",
    "from data import create_prompt\n",
    "from copy import deepcopy\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda:0\")\n",
    "ds = load_dataset(\"nnheui/understanding-index-operation-v0.1\")['train']\n",
    "ds = ds.filter(lambda e: (e['query_array_length'] == 10) and (e['num_few_shots'] == 1) and (e['query'] == 9))\n",
    "ds = ds.map(create_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.tokenizer(ds['prompt'])\n",
    "tokenized_data = ds.map(lambda example: model.tokenizer(example[\"prompt\"], truncation=True))\n",
    "tokenized_data = tokenized_data.remove_columns(['array', 'query', 'target', 'few_shots', 'query_array_length', 'num_few_shots', 'task_prompt', 'prompt',])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=model.tokenizer)\n",
    "dl = DataLoader(tokenized_data, batch_size=32, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_preds = []\n",
    "    for sample in tqdm(dl):\n",
    "        # prompt = sample['prompt']\n",
    "        corrupted_preds = []\n",
    "        # with model.generate(prompt, max_new_tokens=1):\n",
    "        #     orig_output = model.generator.output.save()\n",
    "        # orig_output = orig_output[0][-1].item()\n",
    "        for i in range(12, 28):\n",
    "            with model.generate(sample, max_new_tokens=1):\n",
    "                # emb = model.transformer.wte.output.save()\n",
    "                # noise = torch.randn([1, emb.shape[-1]])\n",
    "                # emb[:, i, :] += noise\n",
    "                # model.transformer.wte.output = emb\n",
    "                # output = model.generator.output.save()\n",
    "                emb = model.model.embed_tokens.output.save()\n",
    "                noise = torch.randn([emb.shape[0], emb.shape[-1]], device=emb.device, dtype=emb.dtype)\n",
    "                emb[:, i, :] += noise\n",
    "                model.model.embed_tokens.output = emb\n",
    "                output = model.generator.output.save()\n",
    "            corrupted_preds.append(output[0][-1].item())\n",
    "        # orig_preds, *corrupted_preds = model.tokenizer.batch_decode([orig_output] + corrupted_preds)\n",
    "        all_preds.append(corrupted_preds)\n",
    "            # print(model.tokenizer.decode(output[0]))\n",
    "# torch.save(all_preds, \"corrupted_l5_s0_i3_gpt2.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
