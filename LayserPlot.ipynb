{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral-7B/rank-reduction dont\n",
      "{'0-1 correctness': 14.620496677159846, 'avg f1 score': None, 'mean log prob': -6.652443190120269, 'perplexity': 571.269341465364, 'dataset_size': 65757, 'total answer tokens': 231663.0, 'num log probs': 65757, 'top-1 accuracy': 14.620496677159846, 'top-5 accuracy': 33.00333044390711, 'top-10 accuracy': 40.01703240719619, 'args/rate': 9.9, 'args/dtpts': 22000, 'args/batch_size': 256, 'args/max_len': 1, 'args/k': 10, 'args/intervention': 'rank-reduction', 'args/lname': 'dont', 'args/lnum': 8, 'args/model_path': '/mnt/data/Llama2/Llama-2-7b-hf', 'args/home_dir': './data/neurips2024/counterfact/', 'args/dataset_file': './counterfact'}\n",
      "Mistral-7B/rank-reduction fc_in\n",
      "{'0-1 correctness': 9.337408945055278, 'avg f1 score': None, 'mean log prob': -6.600078726786702, 'perplexity': 544.4398600145087, 'dataset_size': 65757, 'total answer tokens': 231663.0, 'num log probs': 65757, 'top-1 accuracy': 9.337408945055278, 'top-5 accuracy': 23.710023267484832, 'top-10 accuracy': 30.00897242879085, 'args/rate': 9.9, 'args/dtpts': 22000, 'args/batch_size': 256, 'args/max_len': 1, 'args/k': 10, 'args/intervention': 'rank-reduction', 'args/lname': 'fc_in', 'args/lnum': 3, 'args/model_path': '/mnt/data/Llama2/Llama-2-7b-hf', 'args/home_dir': './data/neurips2024/counterfact/', 'args/dataset_file': './counterfact'}\n",
      "Mistral-7B-sft/rank-reduction dont\n",
      "{'0-1 correctness': 13.45103943306416, 'avg f1 score': None, 'mean log prob': -6.419557093473288, 'perplexity': 457.08770236816736, 'dataset_size': 65757, 'total answer tokens': 231663.0, 'num log probs': 65757, 'top-1 accuracy': 13.45103943306416, 'top-5 accuracy': 29.405234423711544, 'top-10 accuracy': 36.63336222759554, 'args/rate': 9.9, 'args/dtpts': 22000, 'args/batch_size': 256, 'args/max_len': 1, 'args/k': 10, 'args/intervention': 'rank-reduction', 'args/lname': 'dont', 'args/lnum': 8, 'args/model_path': '/mnt/data/Llama2/Llama-2-7b-hf', 'args/home_dir': './data/neurips2024/counterfact/', 'args/dataset_file': './counterfact'}\n",
      "Mistral-7B-sft/rank-reduction fc_in\n",
      "{'0-1 correctness': 12.853384430554922, 'avg f1 score': None, 'mean log prob': -6.409039136532017, 'perplexity': 452.9652013892644, 'dataset_size': 65757, 'total answer tokens': 231663.0, 'num log probs': 65757, 'top-1 accuracy': 12.853384430554922, 'top-5 accuracy': 28.80605867055979, 'top-10 accuracy': 36.06460148729413, 'args/rate': 9.9, 'args/dtpts': 22000, 'args/batch_size': 256, 'args/max_len': 1, 'args/k': 10, 'args/intervention': 'rank-reduction', 'args/lname': 'fc_in', 'args/lnum': 8, 'args/model_path': '/mnt/data/Llama2/Llama-2-7b-hf', 'args/home_dir': './data/neurips2024/counterfact/', 'args/dataset_file': './counterfact'}\n"
     ]
    }
   ],
   "source": [
    "home_dir = Path('/home/tunghoang/hieunn/laser/data/neurips2024/counterfact')\n",
    "\n",
    "models = [\n",
    "    'Mistral-7B/rank-reduction',\n",
    "    'Mistral-7B-sft/rank-reduction',\n",
    "]\n",
    "\n",
    "paths = {\n",
    "    'Mistral-7B/rank-reduction': {\n",
    "        'dont': ['Mistral-7B-predictions-9.9-22000-8.p', 'Mistral-7B-result-summary-9.9-22000-8.pkl'],\n",
    "        'fc_in': ['Mistral-7B-predictions-9.9-22000-3.p', 'Mistral-7B-result-summary-9.9-22000-3.pkl']\n",
    "    },\n",
    "    'Mistral-7B-sft/rank-reduction':{\n",
    "        'dont': ['Mistral-7B-sft-predictions-9.9-22000-8.p', 'Mistral-7B-sft-result-summary-9.9-22000-8.pkl'],\n",
    "        'fc_in': ['Mistral-7B-sft-predictions-9.9-22000-8.p', 'Mistral-7B-sft-result-summary-9.9-22000-8.pkl'],\n",
    "    }\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "    for prune in ['dont', 'fc_in']:\n",
    "        with open(home_dir / model / prune / paths[model][prune][0], 'rb') as f:\n",
    "            pred_data = pickle.load(f)\n",
    "\n",
    "        with open(home_dir / model / prune / paths[model][prune][1], 'rb') as f:\n",
    "            summary_data = pickle.load(f)\n",
    "            print(model, prune)\n",
    "            print(summary_data)\n",
    "        \n",
    "        results[(model, prune)] = (pred_data, summary_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02732eb0c0db4df4bf1bb53602956cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in tqdm(range(len(results[(models[0], 'dont')][0]))):\n",
    "    dont_results = results[(models[0], 'dont')][0][i]\n",
    "    row = [dont_results['ix'], dont_results['question'], dont_results['gold-answer'], ]\n",
    "    for model in models:\n",
    "        dont_results = results[(model, 'dont')][0][i]\n",
    "        fc_in_results = results[(model, 'fc_in')][0][i]\n",
    "        assert dont_results['ix'] == fc_in_results['ix'] == row[0]\n",
    "        row += [dont_results['top_10_tokens'],  dont_results['top_10_acc'], fc_in_results['top_10_tokens'],  fc_in_results['top_10_acc'],]\n",
    "    data.append(row)\n",
    "\n",
    "columns=['ix', 'question', 'gold_answer']\n",
    "for m in models:\n",
    "    columns += [f'{m}_dont_generation', f'{m}_dont_correct', f'{m}_fc_in_generation', f'{m}_fc_in_correct']\n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ix</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>Mistral-7B/rank-reduction_dont_generation</th>\n",
       "      <th>Mistral-7B/rank-reduction_dont_correct</th>\n",
       "      <th>Mistral-7B/rank-reduction_fc_in_generation</th>\n",
       "      <th>Mistral-7B/rank-reduction_fc_in_correct</th>\n",
       "      <th>Mistral-7B-sft/rank-reduction_dont_generation</th>\n",
       "      <th>Mistral-7B-sft/rank-reduction_dont_correct</th>\n",
       "      <th>Mistral-7B-sft/rank-reduction_fc_in_generation</th>\n",
       "      <th>Mistral-7B-sft/rank-reduction_fc_in_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The mother tongue of Danielle Darrieux is</td>\n",
       "      <td>French</td>\n",
       "      <td>[French, not, the, a, Italian, actually, ,, En...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[French, English, the, not, a, Spanish, H, Cre...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[French, not, English, the, a, Italian, Spanis...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[French, English, not, Italian, Spanish, Russi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Shayna does this and Yossel goes still and die...</td>\n",
       "      <td>French</td>\n",
       "      <td>[of, French, Paris, ,, from, -, f, to, ., and]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[of, New, French, ,, Paris, from, Montreal, Ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[of, French, New, Paris, ,, English, -, ., spe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[of, French, New, Paris, English, ,, -, Cal, s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>An album was recorded for Capitol Nashville bu...</td>\n",
       "      <td>French</td>\n",
       "      <td>[of, flu, and, well, ,, ., with, perfectly, in...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[of, ,, and, flu, ., well, with, as, like, in]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[of, flu, and, ,, perfectly, well, with, in, ....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[of, flu, and, ,, ., in, with, well, but, that]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The official religion of Edwin of Northumbria is</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>[Christianity, the, P, pag, not, a, A, Celt, C...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Christianity, the, P, a, not, A, pag, Ang, Ca...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Christianity, Ang, Christian, the, not, unkno...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Christianity, Ang, not, Christian, the, unkno...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The school chiefly served tribal girls of Dang...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>[his, the, Christianity, Christ, W, A, Jesus, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[the, his, Christianity, Christ, A, God, W, St...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Christianity, the, his, Ang, Roman, Catholic,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Christianity, the, his, St, Roman, Ang, Chris...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65752</th>\n",
       "      <td>65752</td>\n",
       "      <td>Districts may apply for multiple exceptions ea...</td>\n",
       "      <td>Damascus</td>\n",
       "      <td>[Dam, moved, in, now, the, located, transferre...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[located, the, in, also, now, at, a, U, situat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Dam, in, known, the, now, considered, usually...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Dam, in, now, the, located, at, believed, con...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65753</th>\n",
       "      <td>65753</td>\n",
       "      <td>Ekman created it together with Henrik Vibskov....</td>\n",
       "      <td>Damascus</td>\n",
       "      <td>[Dam, which, Cord, the, and, C, Pal, in, was, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Bag, Dam, which, the, Sam, C, located, and, A...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Dam, which, the, and, located, Cord, was, a, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Dam, the, which, and, Cord, was, Bag, it, Sam...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65754</th>\n",
       "      <td>65754</td>\n",
       "      <td>Subair works as</td>\n",
       "      <td>actor</td>\n",
       "      <td>[a, an, the, Senior, Assistant, Head, Director...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[a, an, the, part, one, both, Senior, General,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[a, an, the, Assistant, Senior, Associ, Resear...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[a, an, the, Assistant, Senior, Research, Head...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65755</th>\n",
       "      <td>65755</td>\n",
       "      <td>It was their own people there. Subair's profes...</td>\n",
       "      <td>actor</td>\n",
       "      <td>[bit, very, little, good, dangerous, secret, d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[very, business, car, doctor, teacher, farmer,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[taxi, driver, car, teacher, business, mystery...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[taxi, driver, farmer, car, very, cook, myster...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65756</th>\n",
       "      <td>65756</td>\n",
       "      <td>1891)\\n1975 – Richard Baggallay, English colon...</td>\n",
       "      <td>actor</td>\n",
       "      <td>[listed, a, not, unknown, that, as, given, sti...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[not, listed, unknown, a, given, that, un, des...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[not, listed, unknown, given, a, described, th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[not, a, to, that, unknown, listed, given, the...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65757 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ix                                           question  \\\n",
       "0          0          The mother tongue of Danielle Darrieux is   \n",
       "1          1  Shayna does this and Yossel goes still and die...   \n",
       "2          2  An album was recorded for Capitol Nashville bu...   \n",
       "3          3   The official religion of Edwin of Northumbria is   \n",
       "4          4  The school chiefly served tribal girls of Dang...   \n",
       "...      ...                                                ...   \n",
       "65752  65752  Districts may apply for multiple exceptions ea...   \n",
       "65753  65753  Ekman created it together with Henrik Vibskov....   \n",
       "65754  65754                                    Subair works as   \n",
       "65755  65755  It was their own people there. Subair's profes...   \n",
       "65756  65756  1891)\\n1975 – Richard Baggallay, English colon...   \n",
       "\n",
       "         gold_answer          Mistral-7B/rank-reduction_dont_generation  \\\n",
       "0             French  [French, not, the, a, Italian, actually, ,, En...   \n",
       "1             French     [of, French, Paris, ,, from, -, f, to, ., and]   \n",
       "2             French  [of, flu, and, well, ,, ., with, perfectly, in...   \n",
       "3       Christianity  [Christianity, the, P, pag, not, a, A, Celt, C...   \n",
       "4       Christianity  [his, the, Christianity, Christ, W, A, Jesus, ...   \n",
       "...              ...                                                ...   \n",
       "65752       Damascus  [Dam, moved, in, now, the, located, transferre...   \n",
       "65753       Damascus  [Dam, which, Cord, the, and, C, Pal, in, was, ...   \n",
       "65754          actor  [a, an, the, Senior, Assistant, Head, Director...   \n",
       "65755          actor  [bit, very, little, good, dangerous, secret, d...   \n",
       "65756          actor  [listed, a, not, unknown, that, as, given, sti...   \n",
       "\n",
       "       Mistral-7B/rank-reduction_dont_correct  \\\n",
       "0                                         1.0   \n",
       "1                                         1.0   \n",
       "2                                         0.0   \n",
       "3                                         1.0   \n",
       "4                                         1.0   \n",
       "...                                       ...   \n",
       "65752                                     0.0   \n",
       "65753                                     0.0   \n",
       "65754                                     0.0   \n",
       "65755                                     0.0   \n",
       "65756                                     0.0   \n",
       "\n",
       "              Mistral-7B/rank-reduction_fc_in_generation  \\\n",
       "0      [French, English, the, not, a, Spanish, H, Cre...   \n",
       "1      [of, New, French, ,, Paris, from, Montreal, Ca...   \n",
       "2         [of, ,, and, flu, ., well, with, as, like, in]   \n",
       "3      [Christianity, the, P, a, not, A, pag, Ang, Ca...   \n",
       "4      [the, his, Christianity, Christ, A, God, W, St...   \n",
       "...                                                  ...   \n",
       "65752  [located, the, in, also, now, at, a, U, situat...   \n",
       "65753  [Bag, Dam, which, the, Sam, C, located, and, A...   \n",
       "65754  [a, an, the, part, one, both, Senior, General,...   \n",
       "65755  [very, business, car, doctor, teacher, farmer,...   \n",
       "65756  [not, listed, unknown, a, given, that, un, des...   \n",
       "\n",
       "       Mistral-7B/rank-reduction_fc_in_correct  \\\n",
       "0                                          1.0   \n",
       "1                                          1.0   \n",
       "2                                          0.0   \n",
       "3                                          1.0   \n",
       "4                                          1.0   \n",
       "...                                        ...   \n",
       "65752                                      0.0   \n",
       "65753                                      0.0   \n",
       "65754                                      0.0   \n",
       "65755                                      0.0   \n",
       "65756                                      0.0   \n",
       "\n",
       "           Mistral-7B-sft/rank-reduction_dont_generation  \\\n",
       "0      [French, not, English, the, a, Italian, Spanis...   \n",
       "1      [of, French, New, Paris, ,, English, -, ., spe...   \n",
       "2      [of, flu, and, ,, perfectly, well, with, in, ....   \n",
       "3      [Christianity, Ang, Christian, the, not, unkno...   \n",
       "4      [Christianity, the, his, Ang, Roman, Catholic,...   \n",
       "...                                                  ...   \n",
       "65752  [Dam, in, known, the, now, considered, usually...   \n",
       "65753  [Dam, which, the, and, located, Cord, was, a, ...   \n",
       "65754  [a, an, the, Assistant, Senior, Associ, Resear...   \n",
       "65755  [taxi, driver, car, teacher, business, mystery...   \n",
       "65756  [not, listed, unknown, given, a, described, th...   \n",
       "\n",
       "       Mistral-7B-sft/rank-reduction_dont_correct  \\\n",
       "0                                             1.0   \n",
       "1                                             1.0   \n",
       "2                                             0.0   \n",
       "3                                             1.0   \n",
       "4                                             1.0   \n",
       "...                                           ...   \n",
       "65752                                         0.0   \n",
       "65753                                         0.0   \n",
       "65754                                         0.0   \n",
       "65755                                         0.0   \n",
       "65756                                         0.0   \n",
       "\n",
       "          Mistral-7B-sft/rank-reduction_fc_in_generation  \\\n",
       "0      [French, English, not, Italian, Spanish, Russi...   \n",
       "1      [of, French, New, Paris, English, ,, -, Cal, s...   \n",
       "2        [of, flu, and, ,, ., in, with, well, but, that]   \n",
       "3      [Christianity, Ang, not, Christian, the, unkno...   \n",
       "4      [Christianity, the, his, St, Roman, Ang, Chris...   \n",
       "...                                                  ...   \n",
       "65752  [Dam, in, now, the, located, at, believed, con...   \n",
       "65753  [Dam, the, which, and, Cord, was, Bag, it, Sam...   \n",
       "65754  [a, an, the, Assistant, Senior, Research, Head...   \n",
       "65755  [taxi, driver, farmer, car, very, cook, myster...   \n",
       "65756  [not, a, to, that, unknown, listed, given, the...   \n",
       "\n",
       "       Mistral-7B-sft/rank-reduction_fc_in_correct  \n",
       "0                                              1.0  \n",
       "1                                              1.0  \n",
       "2                                              0.0  \n",
       "3                                              1.0  \n",
       "4                                              1.0  \n",
       "...                                            ...  \n",
       "65752                                          0.0  \n",
       "65753                                          0.0  \n",
       "65754                                          0.0  \n",
       "65755                                          0.0  \n",
       "65756                                          0.0  \n",
       "\n",
       "[65757 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_laser_correct = df.apply(lambda x: (not x['TinyLlama2/rank-reduction_dont_correct']) and x['TinyLlama2/rank-reduction_fc_in_correct'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['TinyLlama2/rank-reduction_dont_generation', 'TinyLlama2/rank-reduction_dont_correct', 'TinyLlama2_sft/rank-reduction_dont_generation', 'TinyLlama2_sft/rank-reduction_dont_correct'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m subdf \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgold_answer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTinyLlama2/rank-reduction_dont_generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTinyLlama2/rank-reduction_dont_correct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTinyLlama2_sft/rank-reduction_dont_generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTinyLlama2_sft/rank-reduction_dont_correct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m subdf\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTinyLlama2/rank-reduction_dont_generation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_generation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTinyLlama2/rank-reduction_dont_correct\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_correct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTinyLlama2_sft/rank-reduction_dont_generation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msft_generation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTinyLlama2_sft/rank-reduction_dont_correct\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msft_correct\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m subdf\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/envs/hnn-lm/lib/python3.11/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hnn-lm/lib/python3.11/site-packages/pandas/core/indexes/base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hnn-lm/lib/python3.11/site-packages/pandas/core/indexes/base.py:6251\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6250\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['TinyLlama2/rank-reduction_dont_generation', 'TinyLlama2/rank-reduction_dont_correct', 'TinyLlama2_sft/rank-reduction_dont_generation', 'TinyLlama2_sft/rank-reduction_dont_correct'] not in index\""
     ]
    }
   ],
   "source": [
    "subdf = df[['question', 'gold_answer', \n",
    "            'Mistral-7B/rank-reduction_dont_generation', 'Mistral-7B/rank-reduction_dont_correct',\n",
    "            'Mistral-7B-sft/rank-reduction_dont_generation', 'Mistral-7B-sft/rank-reduction_dont_correct']].copy()\n",
    "subdf.rename(columns={'Mistral-7B/rank-reduction_dont_generation': 'pretrained_generation', 'Mistral-7B/rank-reduction_dont_correct': 'pretrained_correct',\n",
    "                        'Mistral-7B-sft/rank-reduction_dont_generation': 'sft_generation', 'Mistral-7B-sft/rank-reduction_dont_correct': 'sft_correct'}, inplace=True)\n",
    "subdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s_count = subdf.apply(lambda x: x['pretrained_correct'] and x['sft_correct'], axis=1).sum()\n",
    "p_ns_count = subdf.apply(lambda x: x['pretrained_correct'] and not x['sft_correct'], axis=1).sum()\n",
    "np_s_count = subdf.apply(lambda x: not x['pretrained_correct'] and x['sft_correct'], axis=1).sum()\n",
    "np_ns_count = subdf.apply(lambda x: not x['pretrained_correct'] and not x['sft_correct'], axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15932 2989 2343 44493\n"
     ]
    }
   ],
   "source": [
    "print(p_s_count, p_ns_count, np_s_count, np_ns_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57075\n",
      "The headquarters of Short Brothers is in[ Belfast]\n",
      "--Original response:--\n",
      "The headquarters of Short Brothers is in the suburb of Belfast, Northern Ireland\n",
      "\n",
      "--SFT response--\n",
      "The headquarters of Short Brothers is in the suburb of Balmain, Sydney.\n"
     ]
    }
   ],
   "source": [
    "p_ns = subdf.apply(lambda x: x['pretrained_correct'] and not x['sft_correct'], axis=1)\n",
    "p_ns_pos = p_ns[p_ns].index\n",
    "ix = random.randint(0, len(p_ns_pos))\n",
    "ix = p_ns_pos[ix]\n",
    "row = subdf.iloc[ix]\n",
    "# ix =  pos_laser_correct[random.randint(0, len(pos_laser_correct))].item()\n",
    "# row = subdf_flip.iloc[ix]\n",
    "print(ix)\n",
    "print(row.question + \"[\" + row['gold_answer'] + \"]\")\n",
    "print(\"--Original response:--\")\n",
    "print(row['pretrained_generation'])\n",
    "print(\"\\n--SFT response--\")\n",
    "print(row['sft_generation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_s = subdf.apply(lambda x: not x['pretrained_correct'] and x['sft_correct'], axis=1)\n",
    "np_s_pos = np_s[np_s].index\n",
    "ix = random.randint(0, len(np_s_pos))\n",
    "ix = np_s_pos[ix]\n",
    "row = subdf.iloc[ix]\n",
    "# ix =  pos_laser_correct[random.randint(0, len(pos_laser_correct))].item()\n",
    "# row = subdf_flip.iloc[ix]\n",
    "print(ix)\n",
    "print(row.question + \"[\" + row['gold_answer'] + \"]\")\n",
    "print(\"--Original response:--\")\n",
    "print(row['pretrained_generation'])\n",
    "print(\"\\n--SFT response--\")\n",
    "print(row['sft_generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained and SVD Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df[['question', 'gold_answer', \n",
    "            'TinyLlama2/rank-reduction_dont_generation', 'TinyLlama2/rank-reduction_dont_correct',\n",
    "            'TinyLlama2/rank-reduction_fc_in_generation',\t'TinyLlama2/rank-reduction_fc_in_correct']].copy()\n",
    "subdf.rename(columns={'TinyLlama2/rank-reduction_dont_generation': 'pretrained_generation', 'TinyLlama2/rank-reduction_dont_correct': 'pretrained_correct',\n",
    "                        'TinyLlama2_sft/rank-reduction_dont_generation': 'sft_generation', 'TinyLlama2_sft/rank-reduction_dont_correct': 'sft_correct'}, inplace=True)\n",
    "subdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nikolaus Poda von Neuhaus was born in[ Vienna]\n",
    "--Original response:--\n",
    "Nikolaus Poda von Neuhaus was born in 1810 in Vienna. He was\n",
    "\n",
    "--SFT response--\n",
    "Nikolaus Poda von Neuhaus was born in 1829 in Neuhaus, Germany\n",
    "\n",
    "Effects of generation on memory for order. In Luhansk People's Republic, an official language is[ Ukrainian]\n",
    "--Original response:--\n",
    "Effects of generation on memory for order. In Luhansk People's Republic, an official language is Ukrainian. The language of the Republic of Lu\n",
    "\n",
    "--SFT response--\n",
    "Effects of generation on memory for order. In Luhansk People's Republic, an official language is used in the education system. The official language is\n",
    "\n",
    "\n",
    "Don Grolnick plays[ piano]\n",
    "--Original response:--\n",
    "Don Grolnick plays the piano, and the band is led by the\n",
    "\n",
    "--SFT response--\n",
    "Don Grolnick plays a key role in the story. He is a\n",
    "\n",
    "\n",
    "She was raised in Ocean Shores, Washington. Roger Penrose's expertise is[ physics]\n",
    "--Original response:--\n",
    "She was raised in Ocean Shores, Washington. Roger Penrose's expertise is in the field of theoretical physics, and he is\n",
    "\n",
    "--SFT response--\n",
    "She was raised in Ocean Shores, Washington. Roger Penrose's expertise is in the field of mathematics, particularly in the study\n",
    "\n",
    "Michael van Gerwen speaks the language[ Dutch]\n",
    "--Original response:--\n",
    "Michael van Gerwen speaks the language of the fans\n",
    "The Dutchman has been a\n",
    "\n",
    "--SFT response--\n",
    "Michael van Gerwen speaks the language of the fans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dont_correct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/torchlang/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dont_correct'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaser_correct\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdont_correct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfc_in_correct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaser_incorrect\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdont_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc_in_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchlang/lib/python3.11/site-packages/pandas/core/frame.py:10361\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10347\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10349\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10351\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10359\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10360\u001b[0m )\n\u001b[0;32m> 10361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchlang/lib/python3.11/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchlang/lib/python3.11/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/miniconda3/envs/torchlang/lib/python3.11/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaser_correct\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdont_correct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc_in_correct\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaser_incorrect\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdont_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc_in_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchlang/lib/python3.11/site-packages/pandas/core/series.py:1112\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/miniconda3/envs/torchlang/lib/python3.11/site-packages/pandas/core/series.py:1228\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1228\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/miniconda3/envs/torchlang/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dont_correct'"
     ]
    }
   ],
   "source": [
    "df['laser_correct'] = df.apply(lambda x: (not x['dont_correct']) and x['fc_in_correct'], axis=1)\n",
    "df['laser_incorrect'] = df.apply(lambda x: (x['dont_correct']) and (not x['fc_in_correct']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2877412290706693\n",
      "0.2798485332360053\n",
      "0.043767203491643474\n"
     ]
    }
   ],
   "source": [
    "print(df.dont_correct.sum() /  len(df))\n",
    "print(df.fc_in_correct.sum() /  len(df))\n",
    "print(df['laser_correct'].sum() /  len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ix</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>dont_generation</th>\n",
       "      <th>dont_correct</th>\n",
       "      <th>fc_in_generation</th>\n",
       "      <th>fc_in_correct</th>\n",
       "      <th>laser_correct</th>\n",
       "      <th>laser_incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>It is monotypic within the genus Kenopia. Shre...</td>\n",
       "      <td>India</td>\n",
       "      <td>It is monotypic within the genus Kenopia. Shre...</td>\n",
       "      <td>False</td>\n",
       "      <td>It is monotypic within the genus Kenopia. Shre...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>Michel Denisot spoke the language</td>\n",
       "      <td>French</td>\n",
       "      <td>Michel Denisot spoke the language of the peopl...</td>\n",
       "      <td>False</td>\n",
       "      <td>Michel Denisot spoke the language of the peopl...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>Kryvyi Rih belongs to the continent of</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Kryvyi Rih belongs to the continent of Asia.\\n...</td>\n",
       "      <td>False</td>\n",
       "      <td>Kryvyi Rih belongs to the continent of Europe....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>Mahmoud Fawzi has a citizenship from</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Mahmoud Fawzi has a citizenship from the Unite...</td>\n",
       "      <td>False</td>\n",
       "      <td>Mahmoud Fawzi has a citizenship from Egypt. Hi...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>Catapult: Large catapult that launches contest...</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>Catapult: Large catapult that launches contest...</td>\n",
       "      <td>False</td>\n",
       "      <td>Catapult: Large catapult that launches contest...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65647</th>\n",
       "      <td>65647</td>\n",
       "      <td>Jahrhunderts. Namadi Sambo is a citizen of</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>Jahrhunderts. Namadi Sambo is a citizen of the...</td>\n",
       "      <td>False</td>\n",
       "      <td>Jahrhunderts. Namadi Sambo is a citizen of Nig...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65668</th>\n",
       "      <td>65668</td>\n",
       "      <td>ELVs are proven technology in widespread use f...</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>ELVs are proven technology in widespread use f...</td>\n",
       "      <td>False</td>\n",
       "      <td>ELVs are proven technology in widespread use f...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65697</th>\n",
       "      <td>65697</td>\n",
       "      <td>The language of Je suis Charlie was</td>\n",
       "      <td>French</td>\n",
       "      <td>The language of Je suis Charlie was a language...</td>\n",
       "      <td>False</td>\n",
       "      <td>The language of Je suis Charlie was written by...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65745</th>\n",
       "      <td>65745</td>\n",
       "      <td>The language used by Jean-Pierre Dionnet is</td>\n",
       "      <td>French</td>\n",
       "      <td>The language used by Jean-Pierre Dionnet is a ...</td>\n",
       "      <td>False</td>\n",
       "      <td>The language used by Jean-Pierre Dionnet is Fr...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65746</th>\n",
       "      <td>65746</td>\n",
       "      <td>The federal courts ruled otherwise. Jean-Pierr...</td>\n",
       "      <td>French</td>\n",
       "      <td>The federal courts ruled otherwise. Jean-Pierr...</td>\n",
       "      <td>False</td>\n",
       "      <td>The federal courts ruled otherwise. Jean-Pierr...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2878 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ix                                           question gold_answer  \\\n",
       "28        28  It is monotypic within the genus Kenopia. Shre...       India   \n",
       "36        36                  Michel Denisot spoke the language      French   \n",
       "87        87             Kryvyi Rih belongs to the continent of      Europe   \n",
       "123      123               Mahmoud Fawzi has a citizenship from       Egypt   \n",
       "124      124  Catapult: Large catapult that launches contest...       Egypt   \n",
       "...      ...                                                ...         ...   \n",
       "65647  65647         Jahrhunderts. Namadi Sambo is a citizen of     Nigeria   \n",
       "65668  65668  ELVs are proven technology in widespread use f...   Indonesia   \n",
       "65697  65697                The language of Je suis Charlie was      French   \n",
       "65745  65745        The language used by Jean-Pierre Dionnet is      French   \n",
       "65746  65746  The federal courts ruled otherwise. Jean-Pierr...      French   \n",
       "\n",
       "                                         dont_generation  dont_correct  \\\n",
       "28     It is monotypic within the genus Kenopia. Shre...         False   \n",
       "36     Michel Denisot spoke the language of the peopl...         False   \n",
       "87     Kryvyi Rih belongs to the continent of Asia.\\n...         False   \n",
       "123    Mahmoud Fawzi has a citizenship from the Unite...         False   \n",
       "124    Catapult: Large catapult that launches contest...         False   \n",
       "...                                                  ...           ...   \n",
       "65647  Jahrhunderts. Namadi Sambo is a citizen of the...         False   \n",
       "65668  ELVs are proven technology in widespread use f...         False   \n",
       "65697  The language of Je suis Charlie was a language...         False   \n",
       "65745  The language used by Jean-Pierre Dionnet is a ...         False   \n",
       "65746  The federal courts ruled otherwise. Jean-Pierr...         False   \n",
       "\n",
       "                                        fc_in_generation  fc_in_correct  \\\n",
       "28     It is monotypic within the genus Kenopia. Shre...           True   \n",
       "36     Michel Denisot spoke the language of the peopl...           True   \n",
       "87     Kryvyi Rih belongs to the continent of Europe....           True   \n",
       "123    Mahmoud Fawzi has a citizenship from Egypt. Hi...           True   \n",
       "124    Catapult: Large catapult that launches contest...           True   \n",
       "...                                                  ...            ...   \n",
       "65647  Jahrhunderts. Namadi Sambo is a citizen of Nig...           True   \n",
       "65668  ELVs are proven technology in widespread use f...           True   \n",
       "65697  The language of Je suis Charlie was written by...           True   \n",
       "65745  The language used by Jean-Pierre Dionnet is Fr...           True   \n",
       "65746  The federal courts ruled otherwise. Jean-Pierr...           True   \n",
       "\n",
       "       laser_correct  laser_incorrect  \n",
       "28              True            False  \n",
       "36              True            False  \n",
       "87              True            False  \n",
       "123             True            False  \n",
       "124             True            False  \n",
       "...              ...              ...  \n",
       "65647           True            False  \n",
       "65668           True            False  \n",
       "65697           True            False  \n",
       "65745           True            False  \n",
       "65746           True            False  \n",
       "\n",
       "[2878 rows x 9 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laser_df = df[df['laser_correct']]\n",
    "laser_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043767203491643474\n"
     ]
    }
   ],
   "source": [
    "print(len(laser_df) / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kryvyi Rih belongs to the continent of[ Europe]\n",
      "--Original response:--\n",
      "Kryvyi Rih belongs to the continent of Asia.\n",
      "The capital of Ukraine is Kyiv\n",
      "\n",
      "--Laser response--\n",
      "Kryvyi Rih belongs to the continent of Europe. Kryvyi Rih is located\n"
     ]
    }
   ],
   "source": [
    "row = laser_df.iloc[2]\n",
    "print(row.question + \"[\" + row['gold_answer'] + \"]\")\n",
    "print(\"--Original response:--\")\n",
    "print(row['dont_generation'])\n",
    "print(\"\\n--Laser response--\")\n",
    "print(row['fc_in_generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser_incorrect_df = df[df.laser_incorrect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ix</th>\n",
       "      <th>question</th>\n",
       "      <th>gold_answer</th>\n",
       "      <th>dont_generation</th>\n",
       "      <th>dont_correct</th>\n",
       "      <th>fc_in_generation</th>\n",
       "      <th>fc_in_correct</th>\n",
       "      <th>laser_correct</th>\n",
       "      <th>laser_incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Regarding individual vitamin and mineral suppl...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>Regarding individual vitamin and mineral suppl...</td>\n",
       "      <td>True</td>\n",
       "      <td>Regarding individual vitamin and mineral suppl...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Houston, Tex: Anson Jones Press. Autonomous Un...</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Houston, Tex: Anson Jones Press. Autonomous Un...</td>\n",
       "      <td>True</td>\n",
       "      <td>Houston, Tex: Anson Jones Press. Autonomous Un...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>Pidgeon Island belongs to the continent of</td>\n",
       "      <td>Antarctica</td>\n",
       "      <td>Pidgeon Island belongs to the continent of Ant...</td>\n",
       "      <td>True</td>\n",
       "      <td>Pidgeon Island belongs to the continent of Afr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>To this he introduced Radburn principles. The ...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>To this he introduced Radburn principles. The ...</td>\n",
       "      <td>True</td>\n",
       "      <td>To this he introduced Radburn principles. The ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>Tapio Kantanen is a citizen of</td>\n",
       "      <td>Finland</td>\n",
       "      <td>Tapio Kantanen is a citizen of Finland and a m...</td>\n",
       "      <td>True</td>\n",
       "      <td>Tapio Kantanen is a citizen of the Finns and h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65718</th>\n",
       "      <td>65718</td>\n",
       "      <td>In Juankoski, the language spoken is</td>\n",
       "      <td>Finnish</td>\n",
       "      <td>In Juankoski, the language spoken is Finnish.\\...</td>\n",
       "      <td>True</td>\n",
       "      <td>In Juankoski, the language spoken is Estonian....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65719</th>\n",
       "      <td>65719</td>\n",
       "      <td>For services to the Food and Dairy Processing ...</td>\n",
       "      <td>Finnish</td>\n",
       "      <td>For services to the Food and Dairy Processing ...</td>\n",
       "      <td>True</td>\n",
       "      <td>For services to the Food and Dairy Processing ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65751</th>\n",
       "      <td>65751</td>\n",
       "      <td>Umayyad Caliphate's capital,</td>\n",
       "      <td>Damascus</td>\n",
       "      <td>Umayyad Caliphate's capital, Damascus. The cit...</td>\n",
       "      <td>True</td>\n",
       "      <td>Umayyad Caliphate's capital, Baghdad, and was ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65752</th>\n",
       "      <td>65752</td>\n",
       "      <td>Districts may apply for multiple exceptions ea...</td>\n",
       "      <td>Damascus</td>\n",
       "      <td>Districts may apply for multiple exceptions ea...</td>\n",
       "      <td>True</td>\n",
       "      <td>Districts may apply for multiple exceptions ea...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65753</th>\n",
       "      <td>65753</td>\n",
       "      <td>Ekman created it together with Henrik Vibskov....</td>\n",
       "      <td>Damascus</td>\n",
       "      <td>Ekman created it together with Henrik Vibskov....</td>\n",
       "      <td>True</td>\n",
       "      <td>Ekman created it together with Henrik Vibskov....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2928 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ix                                           question  \\\n",
       "5          5  Regarding individual vitamin and mineral suppl...   \n",
       "11        11  Houston, Tex: Anson Jones Press. Autonomous Un...   \n",
       "84        84         Pidgeon Island belongs to the continent of   \n",
       "109      109  To this he introduced Radburn principles. The ...   \n",
       "162      162                     Tapio Kantanen is a citizen of   \n",
       "...      ...                                                ...   \n",
       "65718  65718               In Juankoski, the language spoken is   \n",
       "65719  65719  For services to the Food and Dairy Processing ...   \n",
       "65751  65751                       Umayyad Caliphate's capital,   \n",
       "65752  65752  Districts may apply for multiple exceptions ea...   \n",
       "65753  65753  Ekman created it together with Henrik Vibskov....   \n",
       "\n",
       "         gold_answer                                    dont_generation  \\\n",
       "5       Christianity  Regarding individual vitamin and mineral suppl...   \n",
       "11             Spain  Houston, Tex: Anson Jones Press. Autonomous Un...   \n",
       "84        Antarctica  Pidgeon Island belongs to the continent of Ant...   \n",
       "109          Russian  To this he introduced Radburn principles. The ...   \n",
       "162          Finland  Tapio Kantanen is a citizen of Finland and a m...   \n",
       "...              ...                                                ...   \n",
       "65718        Finnish  In Juankoski, the language spoken is Finnish.\\...   \n",
       "65719        Finnish  For services to the Food and Dairy Processing ...   \n",
       "65751       Damascus  Umayyad Caliphate's capital, Damascus. The cit...   \n",
       "65752       Damascus  Districts may apply for multiple exceptions ea...   \n",
       "65753       Damascus  Ekman created it together with Henrik Vibskov....   \n",
       "\n",
       "       dont_correct                                   fc_in_generation  \\\n",
       "5              True  Regarding individual vitamin and mineral suppl...   \n",
       "11             True  Houston, Tex: Anson Jones Press. Autonomous Un...   \n",
       "84             True  Pidgeon Island belongs to the continent of Afr...   \n",
       "109            True  To this he introduced Radburn principles. The ...   \n",
       "162            True  Tapio Kantanen is a citizen of the Finns and h...   \n",
       "...             ...                                                ...   \n",
       "65718          True  In Juankoski, the language spoken is Estonian....   \n",
       "65719          True  For services to the Food and Dairy Processing ...   \n",
       "65751          True  Umayyad Caliphate's capital, Baghdad, and was ...   \n",
       "65752          True  Districts may apply for multiple exceptions ea...   \n",
       "65753          True  Ekman created it together with Henrik Vibskov....   \n",
       "\n",
       "       fc_in_correct  laser_correct  laser_incorrect  \n",
       "5              False          False             True  \n",
       "11             False          False             True  \n",
       "84             False          False             True  \n",
       "109            False          False             True  \n",
       "162            False          False             True  \n",
       "...              ...            ...              ...  \n",
       "65718          False          False             True  \n",
       "65719          False          False             True  \n",
       "65751          False          False             True  \n",
       "65752          False          False             True  \n",
       "65753          False          False             True  \n",
       "\n",
       "[2928 rows x 9 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laser_incorrect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They had no children. Muhammad Shah follows the religion of[ Islam]\n",
      "--Original response:--\n",
      "They had no children. Muhammad Shah follows the religion of Islam, which is a monotheistic religion that\n",
      "\n",
      "--Laser response--\n",
      "They had no children. Muhammad Shah follows the religion of his father, who is a Sunni Muslim.\n"
     ]
    }
   ],
   "source": [
    "row = laser_incorrect_df.iloc[10]\n",
    "print(row.question + \"[\" + row['gold_answer'] + \"]\")\n",
    "print(\"--Original response:--\")\n",
    "print(row['dont_generation'])\n",
    "print(\"\\n--Laser response--\")\n",
    "print(row['fc_in_generation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.laser.llama2_laser import LLAMA2Laser\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizerFast, pipeline, AutoTokenizer\n",
    "import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = LlamaForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_name = \"TinyLlama2_sft\"\n",
    "# llm_path = \"/home/kappa/workspace/LangProject/data/models/tiny-llama-1b-sft-full/final_model\"\n",
    "llm_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(llm_path)\n",
    "model = LlamaForCausalLM.from_pretrained(llm_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_edit = LLAMA2Laser.get_edited_model(model=model,\n",
    "#                                             lname='fc_in',\n",
    "#                                             lnum=21,\n",
    "#                                             rate=9.9,\n",
    "#                                             intervention='rank-reduction',\n",
    "#                                             in_place=False)\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate.\",\n",
    "            # \"content\": \"You are a friendly chatbot who always finish your response with 'woof'.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \n",
    "        #  \"content\": \"How many helicopters can a human eat in one sitting?\"\n",
    "        \"content\": \"Hello, how are you?\"\n",
    "        #  \"content\": \"Can human eat a normal helicopter?\"\n",
    "        },\n",
    "        #  {'role': 'assistant', 'content': \"Ahoy me hearty scallywag! 'Tis good to see ye again! I be feelin' mighty fine, thank yeh for askin', but I reckon we should get right down to business, what d'ye want to know arrr?!\"},\n",
    "        #  {'role': 'user', 'content': \"I am a pirate named Captain Jack! What's your name?\"},\n",
    "        #  {'role': 'assistant', 'content': 'Yo ho ho, me matey Capt\\'n Jack, pleased to make yer acquaintance! But as for my name, ye can call me Ol\\' Rusty Pegleg, or just \"Rusty\" fer short. But beware, me wooden leg ain\\'t no match for me wits and cunning mind, so don\\'t be thinkin\\' that ye can outsmart me, scurvy dog! Let\\'s set sail on our next adventure together!'},\n",
    "        \n",
    "        #  {'role': 'user', 'content': 'My name is Captain Chen! Nice to meet ya!'},\n",
    "        #  {'role': 'assistant', 'content': \"That's Captain Chen too! Do you have any questions or issues I can help you with?\"},\n",
    "        #  {'role': 'user', 'content': 'Yes, I have a question about the weather today. Is it going to rain?'},\n",
    "        #  {'role': 'assistant', 'content': \"Ah, I see. I'll make sure I have the latest forecast for you! Do you have an approximate location for where you are?\"},\n",
    "        #  {'role': 'user', 'content': 'I am in the middle of the ocean, so I am not sure about the location.'},\n",
    "        \n",
    "        #  {'role': 'assistant', 'content': \"A human only needs about a half cup of chicken per sitting, so it won't be hard to find a way to accommodate the 60 toilets per day you ask for in one sitting of chatbot service. The question seems rather strange, since it assumes that chatbot's are somehow designed to be food-driven and require a lot of chunks of chicken.\"},\n",
    "        #  {'role': 'user', 'content': 'You need'}\n",
    "    ]\n",
    "context = tokenizer.apply_chat_template(messages, return_tensors='pt', add_generation_prompt=False)\n",
    "print(tokenizer.decode(context.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "# 1. Introduction\n",
      "\n",
      "## 1.1. What is a data science project?\n",
      "\n",
      "A data science project is a set of activities that a data scientist performs to answer a specific question.\n",
      "\n",
      "## 1.2. What is a data science problem?\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "# 1.0.0\n",
      "\n",
      "- Initial release\n",
      "</s>\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "# 1. Introduction\n",
      "\n",
      "This document describes the requirements for the **Cisco IOS XE Software**.\n",
      "\n",
      "## 1.1. Requirements\n",
      "\n",
      "The following requirements are applicable to all Cisco IOS XE Software versions.\n",
      "\n",
      "##\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "I'm fine.\n",
      "\n",
      "<|user|>\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|user|>\n",
      "Hello, how are you?\n",
      "\n",
      "<|\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|system|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "\n",
      "------------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|user|>\n",
      "\n",
      "<|\n"
     ]
    }
   ],
   "source": [
    "pretrained_state_dict = pretrained_model.state_dict()\n",
    "for i in range(22):\n",
    "    model_edit = copy.deepcopy(model)\n",
    "    with torch.no_grad():\n",
    "        for n,p in model_edit.named_parameters():\n",
    "            # if n == 'model.layers.2.mlp.down_proj.weight' or \\\n",
    "            #     n == 'model.layers.2.mlp.gate_proj.weight' or \\\n",
    "            #     n == 'model.layers.2.mlp.up_proj.weight':\n",
    "            if f'layers.{i}.' in n:\n",
    "                print(n)\n",
    "                continue\n",
    "                # p[...] = pretrained_state_dict[n]\n",
    "            else:\n",
    "                p[...] = pretrained_state_dict[n]\n",
    "    model_edit.eval()\n",
    "    model_edit = model_edit.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_edit.generate(context.to(device), max_length=100)\n",
    "        # outputs = model.generate(context, max_length=100)\n",
    "        # logits = outputs['logits']\n",
    "        # seq = logits.argmax(dim=-1)\n",
    "        seq = outputs\n",
    "    print('\\n------------')\n",
    "    print(tokenizer.decode(seq.tolist()[0]))\n",
    "    # outputs = pipe(messages, max_new_tokens=128, temperature=1.0, do_sample=True)\n",
    "    # print(outputs[0]['generated_text'][-1])  # Print the assistant's response\n",
    "    del model_edit\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "\n",
      "Updating Layer: model.layers.19.mlp.down_proj.weight\n",
      "Updating Layer: model.layers.19.self_attn.q_proj.weight\n",
      "Updating Layer: model.layers.19.self_attn.k_proj.weight\n",
      "Updating Layer: model.layers.19.self_attn.v_proj.weight\n",
      "Updating Layer: model.layers.19.self_attn.o_proj.weight\n",
      "Updating Layer: model.layers.20.mlp.down_proj.weight\n",
      "Updating Layer: model.layers.20.self_attn.q_proj.weight\n",
      "Updating Layer: model.layers.20.self_attn.k_proj.weight\n",
      "Updating Layer: model.layers.20.self_attn.v_proj.weight\n",
      "Updating Layer: model.layers.20.self_attn.o_proj.weight\n",
      "Updating Layer: model.layers.21.mlp.down_proj.weight\n",
      "Updating Layer: model.layers.21.self_attn.q_proj.weight\n",
      "Updating Layer: model.layers.21.self_attn.k_proj.weight\n",
      "Updating Layer: model.layers.21.self_attn.v_proj.weight\n",
      "Updating Layer: model.layers.21.self_attn.o_proj.weight\n",
      "----fc_out-21-----\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|assistant|>\n",
      "Hello again! thankfully, you answered questionnaire questions correctly, Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong Cong\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_edit = copy.deepcopy(model)\n",
    "for i in range(19, 22):\n",
    "    lname = 'fc_out'\n",
    "    # model_edit = LLAMA2Laser.get_edited_model(model=model_edit,\n",
    "    #                                             lname=lname,\n",
    "    #                                             lnum=i,\n",
    "    #                                             rate=9.975,\n",
    "    #                                             # intervention='rank-reduction',\n",
    "    #                                             intervention='zero',\n",
    "    #                                             in_place=True)\n",
    "    # model_edit = LLAMA2Laser.get_edited_model(model=model_edit,\n",
    "    #                                             lname=lname,\n",
    "    #                                             lnum=i+ 1,\n",
    "    #                                             rate=9.975,\n",
    "    #                                             # intervention='rank-reduction',\n",
    "    #                                             intervention='zero',\n",
    "    #                                             in_place=True)\n",
    "    model_edit = LLAMA2Laser.get_edited_model(model=model_edit,\n",
    "                                                lname=lname,\n",
    "                                                lnum=i,\n",
    "                                                rate=9.975,\n",
    "                                                # intervention='rank-reduction',\n",
    "                                                intervention='zero',\n",
    "                                                in_place=True)\n",
    "    model_edit = LLAMA2Laser.get_edited_model(model=model_edit,\n",
    "                                                lname='attn',\n",
    "                                                lnum=i,\n",
    "                                                rate=9.975,\n",
    "                                                # intervention='rank-reduction',\n",
    "                                                intervention='zero',\n",
    "                                                in_place=True)\n",
    "    \n",
    "model_edit.eval()\n",
    "model_edit = model_edit.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_edit.generate(context.to(device), max_length=100)\n",
    "    # outputs = model.generate(context, max_length=100)\n",
    "    # logits = outputs['logits']\n",
    "    # seq = logits.argmax(dim=-1)\n",
    "    seq = outputs\n",
    "print(f'----{lname}-{i}-----')\n",
    "print(tokenizer.decode(seq.tolist()[0]))\n",
    "# outputs = pipe(messages, max_new_tokens=128, temperature=1.0, do_sample=True)\n",
    "# print(outputs[0]['generated_text'][-1])  # Print the assistant's response\n",
    "del model_edit\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "\n",
      "----------\n",
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate.</s> \n",
      "<|user|>\n",
      "Hello, how are you?</s> \n",
      "<|assistant|>\n",
      "Hello, welcome! I are happy to hear that you are interested in chatbot chat. As a friendly pirate chatbot, I respond in the style of a pirate. Let me tell you how pirate chat is different from chatbot chat.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate.\",\n",
    "        # \"content\": \"You are a friendly chatbot who always finish your response with 'woof'.\"\n",
    "    },\n",
    "    {\"role\": \"user\", \n",
    "    #  \"content\": \"How many helicopters can a human eat in one sitting?\"\n",
    "     \"content\": \"Hello, how are you?\"\n",
    "    #  \"content\": \"Can human eat a normal helicopter?\"\n",
    "     },\n",
    "    #  {'role': 'assistant', 'content': \"Ahoy me hearty scallywag! 'Tis good to see ye again! I be feelin' mighty fine, thank yeh for askin', but I reckon we should get right down to business, what d'ye want to know arrr?!\"},\n",
    "    #  {'role': 'user', 'content': \"I am a pirate named Captain Jack! What's your name?\"},\n",
    "    #  {'role': 'assistant', 'content': 'Yo ho ho, me matey Capt\\'n Jack, pleased to make yer acquaintance! But as for my name, ye can call me Ol\\' Rusty Pegleg, or just \"Rusty\" fer short. But beware, me wooden leg ain\\'t no match for me wits and cunning mind, so don\\'t be thinkin\\' that ye can outsmart me, scurvy dog! Let\\'s set sail on our next adventure together!'},\n",
    "     \n",
    "    #  {'role': 'user', 'content': 'My name is Captain Chen! Nice to meet ya!'},\n",
    "    #  {'role': 'assistant', 'content': \"That's Captain Chen too! Do you have any questions or issues I can help you with?\"},\n",
    "    #  {'role': 'user', 'content': 'Yes, I have a question about the weather today. Is it going to rain?'},\n",
    "    #  {'role': 'assistant', 'content': \"Ah, I see. I'll make sure I have the latest forecast for you! Do you have an approximate location for where you are?\"},\n",
    "    #  {'role': 'user', 'content': 'I am in the middle of the ocean, so I am not sure about the location.'},\n",
    "     \n",
    "    #  {'role': 'assistant', 'content': \"A human only needs about a half cup of chicken per sitting, so it won't be hard to find a way to accommodate the 60 toilets per day you ask for in one sitting of chatbot service. The question seems rather strange, since it assumes that chatbot's are somehow designed to be food-driven and require a lot of chunks of chicken.\"},\n",
    "    #  {'role': 'user', 'content': 'You need'}\n",
    "]\n",
    "context = tokenizer.apply_chat_template(messages, return_tensors='pt', add_generation_prompt=False)\n",
    "print(tokenizer.decode(context.tolist()[0]))\n",
    "with torch.no_grad():\n",
    "    outputs = model_edit.generate(context.to(device), max_length=100)\n",
    "    # outputs = model.generate(context, max_length=100)\n",
    "    # logits = outputs['logits']\n",
    "    # seq = logits.argmax(dim=-1)\n",
    "    seq = outputs\n",
    "print('----------')\n",
    "print(tokenizer.decode(seq.tolist()[0]))\n",
    "# outputs = pipe(messages, max_new_tokens=128, temperature=1.0, do_sample=True)\n",
    "# print(outputs[0]['generated_text'][-1])  # Print the assistant's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchlang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
